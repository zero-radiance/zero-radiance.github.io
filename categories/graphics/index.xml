<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Graphics on Zero Radiance</title>
    <link>https://zero-radiance.github.io/categories/graphics/</link>
    <description>Recent content in Graphics on Zero Radiance</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 17 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://zero-radiance.github.io/categories/graphics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sampling Analytic Participating Media</title>
      <link>https://zero-radiance.github.io/post/analytic-media/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zero-radiance.github.io/post/analytic-media/</guid>
      <description>&lt;p&gt;Rendering of participating media is an important aspect of every modern renderer. When I say participating media, I am not just talking about fog, fire, and smoke. All matter is composed of &lt;a href=&#34;https://courses.lumenlearning.com/boundless-chemistry/chapter/the-structure-of-the-atom/&#34;&gt;atoms&lt;/a&gt;, which can be sparsely (e.g. in a gas) or densely (e.g. in a solid) distributed in space. Whether we consider the particle or the wave nature of &lt;a href=&#34;https://en.wikipedia.org/wiki/Light&#34;&gt;light&lt;/a&gt;, it penetrates all matter (even &lt;a href=&#34;http://webhome.phy.duke.edu/~qelectron/group/group_reading_Born_and_Wolf.pdf&#34;&gt;metals&lt;/a&gt;) to a certain degree and interacts with its atoms along the way. The nature and the degree of &amp;quot;participation&amp;quot; depend on the material in question.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sampling Burley&#39;s Normalized Diffusion Profiles</title>
      <link>https://zero-radiance.github.io/post/sampling-diffusion/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zero-radiance.github.io/post/sampling-diffusion/</guid>
      <description>&lt;p&gt;A couple of years ago, I worked on an &lt;a href=&#34;http://advances.realtimerendering.com/s2018/Efficient%20screen%20space%20subsurface%20scattering%20Siggraph%202018.pdf&#34;&gt;implementation&lt;/a&gt; of Burley&#39;s Normalized Diffusion (a.k.a. Disney SSS). The original &lt;a href=&#34;https://graphics.pixar.com/library/ApproxBSSRDF/paper.pdf&#34;&gt;paper&lt;/a&gt; claims that the CDF is not analytically invertible. I have great respect for both authors, Brent Burley and Per Christensen, so I haven&#39;t questioned their claim for a second. Turns out, &amp;quot;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/connect-creativity/201311/question-everything-everywhere-forever&#34;&gt;Question Everything&lt;/a&gt;&amp;quot; is probably a better mindset.&lt;/p&gt;

&lt;p&gt;I&#39;ve been recently alerted by &lt;a href=&#34;https://twitter.com/stirners_ghost&#34;&gt;@stirners_ghost&lt;/a&gt; on Twitter (thank you!) that the CDF is analytically invertible. In fact, the inversion process is almost trivial, as I will demonstrate below.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Normal Mapping Using the Surface Gradient</title>
      <link>https://zero-radiance.github.io/post/surface-gradient/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zero-radiance.github.io/post/surface-gradient/</guid>
      <description>&lt;p&gt;Realistic rendering at high frame rates remains at the core of real-time computer graphics. High performance and high fidelity are often at odds, requiring clever tricks and approximations to reach the desired quality bar.&lt;/p&gt;

&lt;p&gt;One of the oldest tricks in the book is &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/simulation-of-wrinkled-surfaces/&#34;&gt;bump mapping&lt;/a&gt;. Introduced in 1978 by Jim Blinn, it is a simple way to add mesoscopic detail without increasing the geometric complexity of the scene. Most modern real-time renderers support a variation of this technique called normal mapping. While it&#39;s fast and easy to use, certain operations, such as blending, are &lt;a href=&#34;https://blog.selfshadow.com/publications/blending-in-detail/&#34;&gt;not as simple as they seem&lt;/a&gt;. This is where the so-called &amp;quot;surface gradient framework&amp;quot; comes into play.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alternative Take on the Split Sum Approximation for Cubemap Pre-filtering</title>
      <link>https://zero-radiance.github.io/post/split-sum/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zero-radiance.github.io/post/split-sum/</guid>
      <description>&lt;p&gt;Pre-filtered cubemaps remain an important source of indirect illumination for those of us who still haven&#39;t purchased a Turing graphics card.&lt;/p&gt;

&lt;p&gt;To my knowledge, most implementations use the &lt;a href=&#34;https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf&#34;&gt;split sum approximation&lt;/a&gt; originally introduced by Brian Karis. It is a simple technique, and generally works well given the inherent view independence limitation.&lt;/p&gt;

&lt;p&gt;But why does it work so well, and is there a better way to pre-filter? Let&#39;s find out.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Deep Compositing and Reprojection</title>
      <link>https://zero-radiance.github.io/post/deep-compositing/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://zero-radiance.github.io/post/deep-compositing/</guid>
      <description>&lt;p&gt;Most graphics programmers are familiar with the concept of &lt;a href=&#34;http://jcgt.org/published/0004/02/03/&#34;&gt;alpha&lt;/a&gt;. It has two interpretations - geometrical and optical. The former corresponds to coverage, while the latter refers to opacity.&lt;/p&gt;

&lt;p&gt;Regular compositing assumes non-overlapping objects. Typically, the &lt;a href=&#34;https://graphics.pixar.com/library/Compositing/&#34;&gt;over operator&lt;/a&gt; is used. It implies that one of the objects is in front of the other, and thereby attenuates its contribution to the image.&lt;/p&gt;

&lt;p&gt;This blog post will cover &lt;a href=&#34;https://graphics.pixar.com/library/DeepCompositing/&#34;&gt;deep compositing&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>